## Final Blog
This year, a lot of new features have been added to the tool ‘FilmEditDetection’, as a part of the program Google Summer of Code 2022. The added features mainly focussed on the following dimensions of the tool - Processing Speed, Accuracy of the tool, command line processing interface, operations it can handle, and graphical user interface of the tool.

### Processing Speed
The processing speed is hugely worked on in this version of the tool. It is achieved by basically two approaches - adding a plug-and-play system of deep learning models using a configuration file and parallel processing different sections of a film. 
Support of a configuration file is added to the tool, where the user can specify the type of convolution model to be used while processing the hard cuts and soft cuts. This configuration file comes in the form of ‘config.json’ which stays in the config directory of the parent directory. The JSON file contains fields like encoder type, input image dimension, threshold, and model path for the hard-cut detector field. One can use a lighter CNN encoder like VGG16 to boost the processing speed. The JSON file also contains information about the soft-cut detector. However, it is only restricted to the window size of the input of 3DCNN for now. In the later version, an option will be there to use a different 3DNN encoder, which may also affect the processing speed.

A feature has been added to split the film into different parts, and process the film parallelly in different CPU cores. This is implemented using the python package ‘multiprocessing’. A film is initially divided into N sections according to the number of child processes specified in the configuration file. These film snippets are then processed with the hard-cut detector module in different CPU cores. This improves the processing speed to some extent.


### Accuracy of the tool
In the previous version of the tool, a manually decided threshold was used to classify a pair of frames belonging to different shots in the hard-cut detector module. The classification of frames was primarily performed using a siamese network which checks the similarity between two feature maps of two frames, and if the similarity is less than a threshold, the pair was marked as different shots. However, in this version, a deep learning model based on the siamese network architecture is trained with training data consisting of a pair of frames from the same and different shots. In this way, the deep learning model based on the Siamese network adaptively learns the threshold based on the training data. This improves the classification results and cuts most of the false positives. 

Currently, a siamese network-based model using VGG16 is trained with a pair of frames from the same or different shots. It can be used by setting the ‘threshold’ as ‘Null’ and ‘model_path’ with <path/to/model> in the configuration file. 

### Command Line Processing Interface
A progress bar has been added to the command line interface which depicts the percentage of processing completed in the form of a progress bar. This progress bar also supports the multi-processing of different film sections in different cores. This feature is implemented using the python package ‘tqdm’.

### Operations it can handle
In the previous version, the tool could handle three operations - it could generate a CSV of shot timestamps, a CSV of cut frame index, and a JSON based on the MEP web annotation format. In this version, the tool can handle a new mode of operation - it can generate a ‘.cns’ file which is supported by the film analysis site Cinemetrics. This ‘.cns’ file primarily consists of basic metrics of the film - number of shots, average shot length, and timestamps of individual shots. These metrics are easily computed by calling the cut detector module which provides the cut timestamps. 

Also, there is another feature that allows the user to upload the cut detection result directly to the cinemetrics website. This is implemented by calling a POST request to the cinemetrics server with the cut detection results in the form of a python dictionary. The request is made with the python package ‘requests’.

### Graphical User Interface of the tool
A Graphical User Interface of the tool has been developed which contains all the functionality and features of the command line tool. The interface has been developed using the python framework pyQT.

Like the command-line arguments, the user gets the freedom to select the config file, the operation to be performed, and upload-to-cinemetrics feature. The home window which comes with these options to select comes in a single layout. The user needs to select the Film Path which is present under the Film Path field, and it is a mandatory field. Other fields are set with a default path as soon as the app launches except for the Model Path Field. When the app launches, the Model Path checks if there is a trained deep learning model in the path ./cutdetectorcore/trained-model/cut detector, if not it is kept blank, and the user needs to manually select the model path. If the mode of operation is selected as cinemetrics, it provides the option to select if the cut detection result is to be uploaded to the cinemetrics server, and further, the UI has fields of user information if the previous is selected yes. On filling all the fields, and clicking on submit, a callback is generated which calls the core cut detection file from the backend main.py, specifically the function run_tool(), with all the arguments submitted. Once the process is completed, the user is informed of the saved file path.